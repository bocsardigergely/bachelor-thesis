{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = pd.read_csv(\"data/processed/processed_short.csv\")\n",
    "df_medium = pd.read_csv(\"data/processed/processed_medium.csv\")\n",
    "df_dank = pd.read_csv(\"data/processed/processed_dank.csv\")\n",
    "\n",
    "wv_short = KeyedVectors.load(\"data/df_short.wordvectors\", mmap='r')\n",
    "wv_med = KeyedVectors.load(\"data/df_medium.wordvectors\", mmap='r')\n",
    "wv_dank = KeyedVectors.load(\"data/df_dank.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df_short.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    text = df['text'].to_numpy()\n",
    "    y = df['label']\n",
    "\n",
    "    vectorizer.adapt(text)\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "    wv = KeyedVectors.load(\"data/\" + str(name) + \".wordvectors\", mmap='r')\n",
    "    embeddings_index= {}\n",
    "    for word in voc:\n",
    "        if wv.has_index_for(word):\n",
    "            embeddings_index[word] = wv_short[word]\n",
    "        else:\n",
    "            embeddings_index[word] = np.zeros(100)\n",
    "    \n",
    "    num_tokens = len(voc) + 2\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,)\n",
    "\n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 24 words (0 misses)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x20419352a60>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "create_embedding_layer(df_short)"
   ]
  },
  {
   "source": [
    "# Dope megoldás a convnetre a cikkből, aminek a képét is beraktam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodels = []\n",
    "for kw in (3, 4, 5):    # kernel sizes\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Embedding(len(word_index) + 1,\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False))\n",
    "    submodel.add(Conv1D(FILTERS,    #should be 100 maps\n",
    "                        kw,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        strides=1)) #elvileg egy maxnorm nem árt még bele\n",
    "    submodel.add(GlobalMaxPooling1D())\n",
    "    submodels.append(submodel)\n",
    "big_model = Sequential()\n",
    "big_model.add(Merge(submodels, mode=\"concat\"))\n",
    "big_model.add(Dense(HIDDEN_DIMS))\n",
    "big_model.add(Dropout(P_DROPOUT))\n",
    "big_model.add(Activation('relu'))\n",
    "big_model.add(Dense(1))\n",
    "big_model.add(Activation('sigmoid'))\n",
    "print('Compiling model')\n",
    "big_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = big_model.fit([x_train, x_train, x_train],\r\n",
    "                     y_train,\r\n",
    "                     batch_size=BATCH_SIZE,\r\n",
    "                     epochs=EPOCHS,\r\n",
    "                     validation_data=([x_val, x_val, x_val], y_val),\r\n",
    "                     callbacks=callbacks)"
   ]
  }
 ]
}