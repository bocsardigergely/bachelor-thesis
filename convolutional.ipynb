{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
      "display_name": "Python 3.8.3 64-bit (conda)"
    },
    "colab": {
      "name": "convolutional.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_ApkXQimtmp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import *\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "import requests\n",
        "import io"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vtknUv4mtmr"
      },
      "source": [
        "df_short = pd.read_csv(\"https://raw.githubusercontent.com/bocsardigergely/bachelor-thesis/main/data/processed/processed_short.csv\").sample(frac=1).reset_index(drop=True)\n",
        "df_medium = pd.read_csv(\"https://raw.githubusercontent.com/bocsardigergely/bachelor-thesis/main/data/processed/processed_medium.csv\").sample(frac=1).reset_index(drop=True)\n",
        "df_dank = pd.read_csv(\"https://raw.githubusercontent.com/bocsardigergely/bachelor-thesis/main/data/processed/processed_dank.csv\").sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "df_imdb_full = pd.read_csv(\"data/raw/full_proc_imdb.csv\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOy50udmmtmt"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsdthDI-wXBD"
      },
      "source": [
        "df_joint = pd.concat([df_short, df_medium, df_dank])\n",
        "df_joint = df_joint.reset_index(drop=True)\n",
        "df_train = df_joint.sample(frac=1).reset_index(drop=True)\n",
        "vectorizer.adapt(np.asarray(df_train[\"text\"]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlrQfsQ8mtmu"
      },
      "source": [
        "def build_model():\n",
        "    response = requests.get('https://raw.githubusercontent.com/bocsardigergely/bachelor-thesis/main/universal.npy')\n",
        "    init_matrix = np.load(io.BytesIO(response.content)) \n",
        "    submodels = []\n",
        "    for kw in (3, 4, 5):    # kernel sizes\n",
        "        submodel = Sequential()\n",
        "        submodel.add(Embedding(\n",
        "        20002,\n",
        "        100,\n",
        "        embeddings_initializer=keras.initializers.Constant(init_matrix),\n",
        "        trainable=False,))\n",
        "        submodel.add(Conv1D(100,    \n",
        "                            kw,\n",
        "                            padding='valid',\n",
        "                            activation='relu',\n",
        "                            strides=1, kernel_constraint=max_norm(5.397124926498551)\n",
        "                             ))\n",
        "        submodel.add(GlobalMaxPooling1D())\n",
        "        submodels.append(submodel)\n",
        "\n",
        "    submodel1 = submodels[0]\n",
        "    submodel2 = submodels[1]\n",
        "    submodel3 = submodels[2]\n",
        "\n",
        "    x = add([submodel1.output, submodel2.output, submodel3.output])\n",
        "    \n",
        "    big_model = Sequential()\n",
        "    big_model.add(Dropout(0.09225974322037533))\n",
        "    big_model.add(Dense(25))\n",
        "    big_model.add(Dropout(0.20942239619394942))\n",
        "    big_model.add(Activation('relu'))\n",
        "    big_model.add(Dense(1))\n",
        "    big_model.add(Activation('sigmoid'))\n",
        "\n",
        "    big_model_output = big_model(x)\n",
        "\n",
        "    model = Model([submodel1.input, submodel2.input, submodel3.input], big_model_output)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                    optimizer='adam',\n",
        "                    metrics=['accuracy'])  \n",
        "\n",
        "    print(model.summary())  \n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqnfmMEPmtmv"
      },
      "source": [
        "def train_model(df):\n",
        "    #creating the desired vectors\n",
        "    text = np.asarray(df['text'])\n",
        "    y = df[\"label\"]\n",
        "    text_train, text_test, y_train, y_test = train_test_split(\n",
        "    text, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train = vectorizer(text_train)\n",
        "    X_test = vectorizer(text_test)\n",
        "\n",
        "    model = build_model()\n",
        "\n",
        "    model.fit([X_train, X_train, X_train],\n",
        "                     y_train,\n",
        "                     batch_size=128,\n",
        "                     epochs=5,\n",
        "                     validation_split=0.1,\n",
        "\n",
        "                     )\n",
        "    \n",
        "    loss, accuracy =  model.evaluate([X_test, X_test, X_test], y_test)\n",
        "\n",
        "    print(\"Loss: \", loss)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "        \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCDrRwmJmtmv",
        "outputId": "b0646659-8f0c-4b10-a369-08093b97f4eb"
      },
      "source": [
        "train_model(df_imdb_full)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "embedding_6_input (InputLayer)  [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7_input (InputLayer)  [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8_input (InputLayer)  [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, None, 100)    2000200     embedding_6_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, None, 100)    2000200     embedding_7_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, None, 100)    2000200     embedding_8_input[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, None, 100)    30100       embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, None, 100)    40100       embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, None, 100)    50100       embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 100)          0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_8 (GlobalM (None, 100)          0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 100)          0           global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "                                                                 global_max_pooling1d_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "sequential_11 (Sequential)      (None, 1)            2551        add_2[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 6,123,451\n",
            "Trainable params: 122,851\n",
            "Non-trainable params: 6,000,600\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "282/282 [==============================] - 405s 1s/step - loss: 0.7346 - accuracy: 0.5879 - val_loss: 0.3944 - val_accuracy: 0.8280\n",
            "Epoch 2/5\n",
            "282/282 [==============================] - 356s 1s/step - loss: 0.4170 - accuracy: 0.8127 - val_loss: 0.3301 - val_accuracy: 0.8593\n",
            "Epoch 3/5\n",
            "282/282 [==============================] - 353s 1s/step - loss: 0.3478 - accuracy: 0.8499 - val_loss: 0.3135 - val_accuracy: 0.8660\n",
            "Epoch 4/5\n",
            "282/282 [==============================] - 367s 1s/step - loss: 0.3078 - accuracy: 0.8707 - val_loss: 0.3203 - val_accuracy: 0.8630\n",
            "Epoch 5/5\n",
            "282/282 [==============================] - 355s 1s/step - loss: 0.2574 - accuracy: 0.8949 - val_loss: 0.2998 - val_accuracy: 0.8723\n",
            "313/313 [==============================] - 19s 59ms/step - loss: 0.3125 - accuracy: 0.8656\n",
            "Loss:  0.31252148747444153\n",
            "Accuracy:  0.8655999898910522\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.functional.Functional at 0x1f49462f2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4UIdSfSxOQv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}