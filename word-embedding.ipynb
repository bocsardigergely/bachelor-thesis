{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = pd.read_csv(\"data/processed/processed_short.csv\")\n",
    "df_medium = pd.read_csv(\"data/processed/processed_medium.csv\")\n",
    "df_dank = pd.read_csv(\"data/processed/processed_dank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  label\n",
       "0             thinl role like sachin standing others hit    0.0\n",
       "1                                         think waste rr    0.0\n",
       "2       movie blaxploitation absolutely plot pimp sto...    1.0\n",
       "3       saw movie indian friends christmas day quick ...    0.0\n",
       "4      april good covid vaccin china expect readi cli...    0.0\n",
       "...                                                  ...    ...\n",
       "15559                                   want come online    0.0\n",
       "15560         irloth employ stay home work construct irl    0.0\n",
       "15561   france around march love go film festivals kn...    1.0\n",
       "15562   movie great stars earlier years ingor stevens...    1.0\n",
       "15563                        yeah leave hour least sound    0.0\n",
       "\n",
       "[15564 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>thinl role like sachin standing others hit</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>think waste rr</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>movie blaxploitation absolutely plot pimp sto...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>saw movie indian friends christmas day quick ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>april good covid vaccin china expect readi cli...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15559</th>\n      <td>want come online</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15560</th>\n      <td>irloth employ stay home work construct irl</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15561</th>\n      <td>france around march love go film festivals kn...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15562</th>\n      <td>movie great stars earlier years ingor stevens...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15563</th>\n      <td>yeah leave hour least sound</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>15564 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_joint = pd.concat([df_short, df_medium, df_dank])\n",
    "df_joint = df_joint.reset_index(drop=True)\n",
    "df_train = df_joint.sample(frac=1).reset_index(drop=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_100d(df):\n",
    "    text = np.asarray(df['text'])\n",
    "    y = df['label']\n",
    "    vectorizer.adapt(text)\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    embeddings_index = {}\n",
    "    with open('data/raw/glove.6B.100d.txt',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "    num_tokens = len(voc) + 2\n",
    "    print('num_tokes' + str(num_tokens))\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    \n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\nnum_tokes20002\nConverted 18809 words (1191 misses)\n"
     ]
    }
   ],
   "source": [
    "universal = glove_100d(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "type(universal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embed_matrix', universal)"
   ]
  },
  {
   "source": [
    "# Generating and saving vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    df['list'] = df[\"text\"].str.split()\n",
    "    documents = df[\"list\"].to_numpy()\n",
    "    skipgram = Word2Vec(sentences=documents, vector_size=100, window=5, sg=1)\n",
    "    word_vectors = skipgram.wv\n",
    "    word_vectors.save(\"data/\" + str(name) + \".wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vectors(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vectors(df_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vectors(df_dank)"
   ]
  },
  {
   "source": [
    "# How to load them:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(\"data/df_dank.wordvectors\", mmap='r')"
   ]
  },
  {
   "source": [
    "# legacy example of diy embed layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    #text = df['text'].to_numpy()\n",
    "    text = np.asarray(df['text'])\n",
    "    y = df['label']\n",
    "    vectorizer.adapt(text)\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    print(\"data/\" + str(name) + \".wordvectors\")\n",
    "    wv = KeyedVectors.load(\"data/\" + str(name) + \".wordvectors\", mmap='r')\n",
    "    embeddings_index= {}\n",
    "    for word in voc:\n",
    "        if wv.has_index_for(word):\n",
    "            embeddings_index[word] = wv[word]\n",
    "            \n",
    "        else:\n",
    "            embeddings_index[word] = np.zeros(100)\n",
    "    \n",
    "    num_tokens = len(voc) + 2\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,)\n",
    "\n",
    "    return embedding_layer"
   ]
  }
 ]
}