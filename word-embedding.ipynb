{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = pd.read_csv(\"data/processed/processed_short.csv\")\n",
    "df_medium = pd.read_csv(\"data/processed/processed_medium.csv\")\n",
    "df_dank = pd.read_csv(\"data/processed/processed_dank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  label\n",
       "0       go jurong point crazy available bugis n great...    0.0\n",
       "1                                ok lar joking wif u oni    0.0\n",
       "2       free entry wkly comp win fa cup final tkts st...    1.0\n",
       "3                    u dun say early hor u c already say    0.0\n",
       "4                 nah think goes usf lives around though    0.0\n",
       "...                                                  ...    ...\n",
       "15559  social anxieti there quarantin extrovert socia...    0.0\n",
       "15560       hold door open girl amato fato gentleman psi    0.0\n",
       "15561  babi like bababamom babi gorgeous say mummi ba...    0.0\n",
       "15562  movi terriblefrom lack leadershid prepar donal...    0.0\n",
       "15563  oofer gang boi fuck austria wolf fuck town sig...    0.0\n",
       "\n",
       "[15564 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>go jurong point crazy available bugis n great...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ok lar joking wif u oni</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>free entry wkly comp win fa cup final tkts st...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>u dun say early hor u c already say</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nah think goes usf lives around though</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15559</th>\n      <td>social anxieti there quarantin extrovert socia...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15560</th>\n      <td>hold door open girl amato fato gentleman psi</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15561</th>\n      <td>babi like bababamom babi gorgeous say mummi ba...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15562</th>\n      <td>movi terriblefrom lack leadershid prepar donal...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15563</th>\n      <td>oofer gang boi fuck austria wolf fuck town sig...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>15564 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_joint = pd.concat([df_short, df_medium, df_dank])\n",
    "df_joint = df_joint.reset_index(drop=True)\n",
    "df_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_100d(df):\n",
    "    text = np.asarray(df['text'])\n",
    "    y = df['label']\n",
    "    vectorizer.adapt(text)\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    embeddings_index = {}\n",
    "    with open('data/raw/glove.6B.100d.txt',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "    num_tokens = len(voc) + 2\n",
    "    print('num_tokes' + str(num_tokens))\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False\n",
    "    )\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\nnum_tokes20002\nConverted 18809 words (1191 misses)\n"
     ]
    }
   ],
   "source": [
    "universal = glove_100d(df_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( universal, open( \"universal_embed_layer.p\", \"wb\" ) )"
   ]
  },
  {
   "source": [
    "# Generating and saving vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    df['list'] = df[\"text\"].str.split()\n",
    "    documents = df[\"list\"].to_numpy()\n",
    "    skipgram = Word2Vec(sentences=documents, vector_size=100, window=5, sg=1)\n",
    "    word_vectors = skipgram.wv\n",
    "    word_vectors.save(\"data/\" + str(name) + \".wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vectors(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vectors(df_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vectors(df_dank)"
   ]
  },
  {
   "source": [
    "# How to load them:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(\"data/df_dank.wordvectors\", mmap='r')"
   ]
  },
  {
   "source": [
    "# legacy example of diy embed layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    #text = df['text'].to_numpy()\n",
    "    text = np.asarray(df['text'])\n",
    "    y = df['label']\n",
    "    vectorizer.adapt(text)\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    print(\"data/\" + str(name) + \".wordvectors\")\n",
    "    wv = KeyedVectors.load(\"data/\" + str(name) + \".wordvectors\", mmap='r')\n",
    "    embeddings_index= {}\n",
    "    for word in voc:\n",
    "        if wv.has_index_for(word):\n",
    "            embeddings_index[word] = wv[word]\n",
    "            \n",
    "        else:\n",
    "            embeddings_index[word] = np.zeros(100)\n",
    "    \n",
    "    num_tokens = len(voc) + 2\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,)\n",
    "\n",
    "    return embedding_layer"
   ]
  }
 ]
}